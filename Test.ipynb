{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 11:52:46.397573: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 11:52:47.222965: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-18 11:52:47.223066: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-18 11:52:47.223077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# save the final model to file\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "\t# save model\n",
    "\tmodel.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 6s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 11:52:59.606431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:52:59.618346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:52:59.620003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:52:59.622426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 11:52:59.623340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:52:59.624986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:52:59.626541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:53:00.321819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:53:00.323578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:53:00.325052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 11:53:00.326448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-05-18 11:53:02.808318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n"
     ]
    }
   ],
   "source": [
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 72.990\n"
     ]
    }
   ],
   "source": [
    "# evaluate the deep model on the test dataset\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t# load model\n",
    "\tmodel = load_model('final_model.h5')\n",
    "\t# evaluate model on test dataset\n",
    "\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 performance: \n",
      "> 74.840\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See if normalizing the data to have zero mean and standard deviation 1 improves performance\n",
    "\n",
    "def prep_pixels_2(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# calculate mean and standard deviation\n",
    "\ttrain_mean, train_std = train_norm.mean(), train_norm.std()\n",
    "\ttest_mean, test_std = test_norm.mean(), test_norm.std()\n",
    "\t# global standardization of pixels\n",
    "\ttrain_norm = (train_norm - train_mean) / train_std\n",
    "\ttest_norm = (test_norm - test_mean) / test_std\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness_1task():\n",
    "\t# load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_1task.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness_1task()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def run_test_harness_load_1task():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels_2(trainX, testX)\n",
    "\t# load model\n",
    "\tmodel = load_model('final_model_1task.h5')\n",
    "\t# evaluate model on test dataset\n",
    "\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\n",
    "# entry point, run the test harness\n",
    "print(\"Task 1 performance: \")\n",
    "run_test_harness_load_1task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 2, Replace the SGD + momentum optimizer with Adam and then AdamW. Do these optimizers lead to better performance and/or faster convergence?\n",
    "import keras.optimizers\n",
    "\n",
    "\n",
    "def define_model_2task(optimizer='SGD'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\tif optimizer == 'SGD':\n",
    "\t\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\telif optimizer == 'Adam':\n",
    "\t\topt = keras.optimizers.Adam()\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "\n",
    "def run_test_harness_2task_adam():\n",
    "\t# load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model_2task('Adam')\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_2task_adam.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness_2task_adam()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "\n",
    "\n",
    "def run_test_harness_load_2task_adam():\n",
    "\t# load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # load model\n",
    "    model = load_model('final_model_2task_adam.h5')\n",
    "    # evaluate model on test dataset\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "# entry point, run the test harness\n",
    "print(\"Task 2 Adam performance: \")\n",
    "\n",
    "run_test_harness_load_2task_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 AdamW performance: \n",
      "> 79.880\n"
     ]
    }
   ],
   "source": [
    "# AdamW\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "def define_model_2task(optimizer='SGD'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    if optimizer == 'SGD':\n",
    "        opt = SGD(lr=0.001, momentum=0.9)\n",
    "    elif optimizer == 'Adam':\n",
    "        opt = keras.optimizers.Adam()\n",
    "    elif optimizer == 'AdamW':\n",
    "        opt = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\t\t\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "\n",
    "def run_test_harness_2task_adam2():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model_2task('AdamW')\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_2task_adamW.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness_2task_adam2()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "\n",
    "def run_test_harness_load_2task_adamW():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # load model\n",
    "    model = load_model('final_model_2task_adamW.h5')\n",
    "    # evaluate model on test dataset\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "# entry point, run the test harness\n",
    "print(\"Task 2 AdamW performance: \")\n",
    "run_test_harness_load_2task_adamW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers import Activation\n",
    "\n",
    "# task 4 Check if changing the order to Batch Norm then Dropout has an effect on performance.\n",
    "# Also check if the Dropout and Batch Norm are complementary ie having both Dropout and Batch Norm in the network\n",
    "# is better or worse than having a network that just has one of these regularization techniques.\n",
    "def define_model_4(optimizer='SGD', order='BD', use_both=True):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    if use_both or order == 'BD':\n",
    "        model.add(BatchNormalization())\n",
    "    if use_both or order == 'DB':\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "    if use_both or order == 'BD':\n",
    "        model.add(BatchNormalization())\n",
    "    if use_both or order == 'DB':\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "    if use_both or order == 'BD':\n",
    "        model.add(BatchNormalization())\n",
    "    if use_both or order == 'DB':\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    if optimizer == 'SGD':\n",
    "        opt = SGD(lr=0.001, momentum=0.9)\n",
    "    elif optimizer == 'Adam':\n",
    "        opt = keras.optimizers.Adam()\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm then Dropout:\n",
      "> 82.720\n",
      "Dropout then BatchNorm:\n",
      "> 82.150\n"
     ]
    }
   ],
   "source": [
    "def run_test_harness_4BD():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model_4(optimizer='Adam', order='BD', use_both=True)\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_task4BD.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "#run_test_harness_4BD()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "\n",
    "def run_test_harness_load_4_BD():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # load model\n",
    "    model = load_model('final_model_task4BD.h5')\n",
    "    # evaluate model on test dataset\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "print(\"BatchNorm then Dropout:\")\n",
    "run_test_harness_load_4_BD()\n",
    "\n",
    "def run_test_harness_4_DB():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model_4(optimizer='Adam', order='BD', use_both=True)\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_task4_DB.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "#run_test_harness_4_DB()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "\n",
    "def run_test_harness_load_4_DB():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # load model\n",
    "    model = load_model('final_model_task4_DB.h5')\n",
    "    # evaluate model on test dataset\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "print(\"Dropout then BatchNorm:\")\n",
    "run_test_harness_load_4_DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness_4_B_only():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model_4(optimizer='Adam', order='BD', use_both=True)\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_task4_B_only.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness_4_B_only()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "\n",
    "def run_test_harness_load_4_B_only():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # load model\n",
    "    model = load_model('final_model_task4_B_only.h5')\n",
    "    # evaluate model on test dataset\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only BatchNorm:\n",
      "> 82.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 13:33:44.379263: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_13/dropout_16/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only Dropout:\n",
      "> 81.880\n"
     ]
    }
   ],
   "source": [
    "print(\"Only BatchNorm:\")\n",
    "run_test_harness_load_4_B_only()\n",
    "\n",
    "\n",
    "def run_test_harness_4_D_only():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model_4(optimizer='Adam', order='BD', use_both=True)\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "    # save model\n",
    "    model.save('final_model_task4_D_only.h5')\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness_4_D_only()\n",
    "\n",
    "# evaluate the deep model on the test dataset\n",
    "\n",
    "def run_test_harness_load_4_D_only():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    # load model\n",
    "    model = load_model('final_model_task4_D_only.h5')\n",
    "    # evaluate model on test dataset\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Only Dropout:\")\n",
    "run_test_harness_load_4_D_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model):\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_2(trainX, testX)\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> Test Accuracy: %.3f' % (acc * 100.0))\n",
    "\n",
    "    #loss = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> Test Loss: %.3f' % _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Test Accuracy: 82.720\n",
      "> Test Loss: 1.194\n"
     ]
    }
   ],
   "source": [
    "# 1. Learning Rate Warm-Up + Cosine Annealing:\n",
    "#Best model so far with accuracy of 82.72 so far\n",
    "model = load_model('final_model_task4BD.h5')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "lr_schedule = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate,\n",
    "    maximal_learning_rate=0.1,\n",
    "    step_size=2000,\n",
    "    scale_fn=lambda x: 1.,\n",
    "    scale_mode=\"cycle\",\n",
    "    name=\"MyCyclicScheduler\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 14:10:21.456580: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_8/dropout_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 14s 7ms/step - loss: 2.4403 - accuracy: 0.1002 - lr: 0.1000\n",
      "> Test Accuracy: 10.000\n",
      "> Test Loss: 2.310\n"
     ]
    }
   ],
   "source": [
    "#Best model so far with accuracy of 82.72 so far\n",
    "model = load_model('final_model_task4BD.h5')\n",
    "\n",
    "\n",
    "# 2. Step Decay:\n",
    "import numpy as np\n",
    "\n",
    "def step_decay_schedule(initial_lr=0.1, decay_factor=0.75, step_size=10):\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    return tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=0.1, decay_factor=0.75, step_size=2)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "trainX, trainY, testX, testY = load_dataset()\n",
    "# prepare pixel data\n",
    "trainX, testX = prep_pixels_2(trainX, testX)\n",
    "\n",
    "model.fit(trainX, trainY, callbacks=[lr_sched])\n",
    "\n",
    "\n",
    "evaluate_model(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_addons.optimizers' has no attribute 'sgdr_learning_rate_cycle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_25178/4205299588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m lr_schedule = tfa.optimizers.sgdr_learning_rate_cycle(min_lr=1e-5,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                       \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                       \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_addons.optimizers' has no attribute 'sgdr_learning_rate_cycle'"
     ]
    }
   ],
   "source": [
    "# 3. Cosine Annealing with Restarts (SGDR):\n",
    "#Best model so far with accuracy of 82.72 so far\n",
    "model = load_model('final_model_task4BD.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_schedule = tfa.optimizers.sgdr_learning_rate_cycle(min_lr=1e-5,\n",
    "                                                      max_lr=1e-2,\n",
    "                                                      cycle_length=1000,\n",
    "                                                      lr_decay=0.9,\n",
    "                                                      mult_factor=1.5)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
